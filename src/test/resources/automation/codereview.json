[{"topicKey":"code-review-general","key":"What is code review ?","startLine":"Code review","value":"<p>Code review is a software quality assurance activity in which one or several humans check a program mainly by viewing and reading parts of its source code, and they do so after implementation or as an interruption of implementation. At least one of the humans must not be the code's author. The humans performing the checking, excluding the author, are called \"reviewers\".</p>"},
{"topicKey":"code-review-types","key":"Types of code review","startLine":"Formal Inspections","value":"<p>For historical reasons, &ldquo;formal&rdquo; reviews are usually called &ldquo;inspections.&rdquo; This is a hold-over from Michael Fagan&rsquo;s seminal 1976 study at IBM regarding the efficacy of peer reviews. He tried many combinations of variables and came up with a procedure for reviewing up to 250 lines of prose or source code.<br /> After 800 iterations he came up with a formalized inspection strategy and to this day you can pay him to tell you about it (company name: Fagan Associates). His methods were further studied and expanded upon by others, most notably Tom Gilb and Karl Wiegers.<br />&nbsp; In general, a &ldquo;formal&rdquo; review refers to a heavy-process review with three to six participants meeting together in one room with printouts and/or a projector. Someone is the &ldquo;moderator&rdquo; or &ldquo;controller&rdquo; and acts as the organizer, keeps everyone on task, controls the pace of the review, and acts as arbiter of disputes. Everyone reads through the materials beforehand to properly prepare for the meeting.<br /> Each participant will be assigned a specific &ldquo;role.&rdquo; A &ldquo;reviewer&rdquo; might be tasked with critical analysis while an &ldquo;observer&rdquo; might be called in for domain-specific advice or to learn how to do reviews properly. In a Fagan Inspection, a &ldquo;reader&rdquo; looks at source code only for comprehension &ndash; not for critique &ndash; and presents this to the group. This separates what the author intended from what is actually presented; often the author himself is able to pick out defects given this thirdparty description.<br /> When defects are discovered in a formal review, they are usually recorded in great detail. Besides the general location of the error in the code, they include details such as severity (e.g. major, minor), type (e.g. algorithm, documentation, data-usage, error-handling), and phase-injection (e.g. developer error, design oversight, requirements mistake). Typically this information is kept in a database so defect metrics can be analyzed from many angles and possibly compared to similar metrics from QA.<br /> Formal inspections also typically record other metrics such as individual time spent during pre-meeting reading and during the meeting itself, lines-of-code inspection rates, and problems encountered with the process itself. These numbers and comments are examined periodically in process-improvement meetings; Fagan Inspections go one step further and requires a process-rating questionnaire after each meeting to help with the improvement step.</p>"},
{"topicKey":"code-review-types","key":"Type1 of code review","startLine":"Over-the-shoulder reviews","value":"<p>This is the most common and informal of code reviews. An &ldquo;overthe-shoulder&rdquo; review is just that &ndash; a developer standing over the author&rsquo;s workstation while the author walks the reviewer through a set of code changes.<br /> Typically the author &ldquo;drives&rdquo; the review by sitting at the keyboard and mouse, opening various files, pointing out the changes and explaining why it was done this way. The author can present the changes using various tools and even run back and forth between changes and other files in the project. If the reviewer sees something amiss, they can engage in a little &ldquo;spot pair-programming&rdquo; as the author writes the fix while the reviewer hovers. Bigger changes where the reviewer doesn&rsquo;t need to be involved are taken off-line.<br /> With modern desktop-sharing software a so-called &ldquo;over-theshoulder&rdquo; review can be made to work over long distances. This complicates the process because you need schedule these sharing meetings and communicate over the phone. Standing over a shoulder allows people to point, write examples, or even go to a whiteboard for discussion; this is more difficult over the Internet.<br /> The most obvious advantage of over-the-shoulder reviews is simplicity in execution. Anyone can do it, any time, without training. It can also be deployed whenever you need it most &ndash; an especially complicated change or an alteration to a &ldquo;stable&rdquo; code branch.<br /> As with all in-person reviews, over-the-shoulders lend themselves to learning and sharing between developers and gets people to interact in person instead of hiding behind impersonal email and instantmessages. You naturally talk more when you can blurt out and idea rather than making some formal &ldquo;defect&rdquo; in a database somewhere.<br /> Unfortunately, the informality and simplicity of the process also leads to a mountain of shortcomings. First, this is not an enforceable process &ndash; there&rsquo;s nothing that lets a manager know whether all code changes are being reviewed. In fact, there are no metrics, reports, or tools that measure anything at all about the process.<br /> Second, it&rsquo;s easy for the author to unintentionally miss a change. Countless times we&rsquo;ve observed a review that completes, the author checks in his changes, and when he sees the list of files just checked in he says &ldquo;Oh, did I change that one?&rdquo; Too late!</p>"},
{"topicKey":"code-review-types","key":"Type2 of code review","startLine":"E-mail pass-around reviews","value":"<p>This is the second-most common form of informal code review, and the technique preferred by most open-source projects. Here, whole files or changes are packaged up by the author and sent to reviewers via e-mail. Reviewers examine the files, ask questions and discuss with the author and other developers, and suggest changes.<br /> The hardest part of the e-mail pass-around is in finding and collecting the files under review. On the author&rsquo;s end, he has to figure out how to gather the files together. For example, if this is a review of changes being proposed to check into version control, the user has to identify all the files added, deleted, and modified, copy them somewhere, then download the previous versions of those files (so reviewers can see what was changed), and organize the files so the reviewers know which files should be compared with which others. On the reviewing end, reviewers have to extract those files from the email and generate differences between each.<br /> The version control system can be of some assistance. Typically that system can report on which files have been altered and can be made to extract previous versions. Although some people write their own scripts to collect all these files, most use commercial tools that do the same thing and can handle the myriad of corner-cases arising from files in various states and client/server configurations.<br /> The version control system can also assist by sending the e-mails out automatically. For example, a version control server-side &ldquo;checkin&rdquo; trigger can send e-mails depending on who checked in the code (e.g. the lead developer of each group reviews code from members of that group) and which files were changed (e.g. some files are &ldquo;owned&rdquo; by a user who is best-qualified to review the changes). The automation is helpful, but for many code review processes you want to require reviews before check-in, not after.<br /> Like over-the-shoulder reviews, e-mail pass-arounds are easy to implement, although more time-consuming because of the filegathering. But unlike over-the-shoulder reviews, they work equally well with developers working across the hall or across an ocean. And you eliminate the problem of the authors coaching the reviewers through the changes.<br /> Another unique advantage of e-mail pass-arounds is the ease in which other people can be brought into the review. Perhaps there is a domain expert for a section of code that a reviewer wants to get an opinion from. Perhaps the reviewer wants to defer to another reviewer. Or perhaps the e-mail is sent to many people at once, and those people decide for themselves who are best qualified to review which parts of the code. This inclusiveness is difficult with in-person reviews and with formal inspections where all participants need to be invited to the meeting in advance.<br /> Yet another advantage of e-mail pass-arounds is they don&rsquo;t knock reviewers out of &ldquo;the zone.&rdquo; It&rsquo;s well established that it takes a developer 15 minutes to get into &ldquo;the zone&rdquo; where they are immersed in their work and are highly productive3. Even just asking a developer for a review knocks him out of the zone &ndash; even if the response is &ldquo;I&rsquo;m too busy.&rdquo; With e-mails, reviewers can work during a self-prescribed break so they can stay in the zone for hours at a time.<br /> There are several important drawbacks to the e-mail pass-around review method. The biggest is that for all but the most trivial reviews, it can rapidly become difficult to track the various threads of conversation and code changes. With several discussions concerning a few different areas of the code, possibly inviting other developers to the fray, it&rsquo;s hard to track what everyone&rsquo;s saying or whether the group is getting to a consensus.</p>"},
{"topicKey":"code-review-types","key":"Type3 of code review","startLine":"Tool-Assisted reviews","value":"<p>This refers to any process where specialized tools are used in all aspects of the review: collecting files, transmitting and displaying files, commentary and defects among all participants, collecting metrics, and giving product managers and administrators some control over the workflow. There are several key elements that must be present in a review tool if it is going to solve the major problems with other types of review:<br />- <span style=\"text-decoration: underline;\">Automated File Gathering</span>:<br /> As we discussed in the e-mail pass-around section, you can&rsquo;t have developers spending time manually gathering files and differences for review. A tool must integrate with your version control system to extract current and previous versions so reviewers can easily see the changes under review.<br /> Ideally the tool can do this both with local changes not yet checked into version control and with already-checked-in changes (e.g. by date, label, branch, or unique change-list number). Even if you&rsquo;re not doing both types of review today, you&rsquo;ll want the option in the future.<br />- <span style=\"text-decoration: underline;\">Combined Display</span>: <br />Differences, Comments, Defects One of the biggest time-sinks with any type of review is in reviewers and developers having to associate each sub-conversation with a particular file and line number. The tool must be able to display files and before/after file differences in such a manner that conversations are threaded and no one has to spend time cross-referencing comments, defects, and source code.<br /> <span style=\"text-decoration: underline;\">- Automated Metrics Collection:</span> <br />On one hand, accurate metrics are the only way to understand your process and the only way to measure the changes that occur when you change the process. On the other hand, no developer wants to review code while holding a stopwatch and wielding linecounting tools.<br /> A tool that automates collection of key metrics is the only way to keep developers happy (i.e. no extra work for them) and get meaningful metrics on your process. A full discussion of review metrics and what they mean appears in another essay, but your tool should at least collect these three rates: kLOC/hour (inspection rate), defects/hour (defect rate), and defects/kLOC (defect density).<br />- <span style=\"text-decoration: underline;\">Review Enforcement:<br /></span> Almost all other types of review suffer from the problem of product managers not knowing whether developers are reviewing all code changes or whether reviewers are verifying that defects are indeed fixed and didn&rsquo;t cause new defects. A tool should be able to enforce this workflow at least at a reporting level (for passive workflow enforcement) and at best at the version control level (with serverside triggers that enforce workflow at the version control level).<br />- <span style=\"text-decoration: underline;\">Clients and Integrations:<br /></span> Some developers like command-line tools. Others prefer integrations with IDE&rsquo;s and version control GUI clients. Administrators like zero-installation web clients. It&rsquo;s important that a tool supports many ways to read and write data in the system. Developer tools also have a habit of needing to be integrated with other tools. Version control clients are inside IDE&rsquo;s. Issuetrackers are correlated with version control changes. Similarly, your review tool needs to integrate with your other tools &ndash; everything from IDE&rsquo;s and version control clients to metrics and reports. A bonus is a tool that exposes a public API so you can make customizations and detailed integrations yourself.<br /> If your tool satisfies this list of requirements, you&rsquo;ll have the benefits of e-mail pass-around reviews (works with multiple, possiblyremote developers, minimizes interruptions) but without the problems of no workflow enforcement, no metrics, and wasting time with file/difference packaging, delivery, and inspection. The drawback of any tool-assisted review is cost &ndash; either in cash for a commercial tool or as time if developed in-house. You also need to make sure the tool is flexible enough to handle your specific code review process; otherwise you might find the tool driving your process instead of vice-versa. Although tool-assisted reviews can solve the problems that plague typical code reviews, there is still one other technique that,while not often used, has the potential to find even more defects than standard code review.</p>"},
{"topicKey":"code-review-types","key":"Type4 of code review","startLine":"Pair-Programming","value":"<p>Most people associate pair-programming with XP5 and agile development in general, but it&rsquo;s also a development process that incorporates continuous code review.<br /> &nbsp; Pair-programming is two developers writing code at a single workstation with only one developer typing at a time and continuous free-form discussion and review.<br /> Studies of pair-programming have shown it to be very effective at both finding bugs and promoting knowledge transfer. And some developers really enjoy doing it.<br /> &nbsp; There&rsquo;s a controversial issue about whether pair-programming reviews are better, worse, or complementary to more standard reviews.<br /> &nbsp; The reviewing developer is deeply involved in the code, giving great thought to the issues and consequences arising from different implementations. On the one hand this gives the reviewer lots of inspection time and a deep insight into the problem at hand, so perhaps this means the review is more effective. On the other hand, this closeness is exactly what you don&rsquo;t want in a reviewer; just as no author can see all typos in his own writing, a reviewer too close to the code cannot step back and critique it from a fresh and unbiased position. Some people suggest using both techniques &ndash; pairprogramming for the deep review and a follow-up standard review for fresh eyes. Although this takes a lot of developer time to implement, it would seem that this technique would find the greatest number of defects. We&rsquo;ve never seen anyone do this in practice. The single biggest complaint about pair-programming is that it takes too much time. &nbsp; Rather than having a reviewer spend 15-30 minutes reviewing a change that took one developer a few days to&nbsp;make, in pair-programming you have two developers on the task the entire time.<br /> &nbsp; Some developers just don&rsquo;t like pair-programming; it depends on the disposition of the developers and who is partnered with whom. Pair-programming also does not address the issue of remote developers.</p>"}]
